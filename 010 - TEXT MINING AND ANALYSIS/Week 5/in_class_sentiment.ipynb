{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc25de6c54415b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Blog: [Sentiment Analysis 101](https://blog.pjjop.org/sentiment-analysis-101/?fbclid=IwAR3C1AhPnIxIm7_uZlbbXkS3lbnPL71IOPMAvd1eDqHk-PFNzAm5nEfIT1M)\n",
    "Drive: [Dataset and Notebook](https://drive.google.com/drive/folders/1KhpTSekIG9VOLudh9UoTQQHu1mVK78t8?fbclid=IwAR0y8M99s2YXcXD87UyhAoaSLSXZ90totktEpMFrrieAdvM8oKvrT90G6Y4)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b017836de656d896"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-06T07:02:24.137864200Z",
     "start_time": "2023-08-06T07:02:24.126858300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# GZIP implementation\n",
    "from collections import Counter\n",
    "import gzip\n",
    "import multiprocessing as mp\n",
    "import os.path as op\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "# import dataloader \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Pythainlp\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"wisesight_sentiment\")\n",
    "# get df from dataset\n",
    "df_train = pd.DataFrame(dataset['train']).rename(columns={'category': 'label'})\n",
    "df_valid = pd.DataFrame(dataset['validation']).rename(columns={'category': 'label'})\n",
    "df_test = pd.DataFrame(dataset['test']).rename(columns={'category': 'label'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T07:00:01.868984Z",
     "start_time": "2023-08-06T06:59:57.674670300Z"
    }
   },
   "id": "3e039978d373e31c"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "# check data\n",
    "df_train.head().to_clipboard()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T07:07:28.428763600Z",
     "start_time": "2023-08-06T07:07:28.391635Z"
    }
   },
   "id": "ed2d3af0270ce370"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch Implementation RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baefb339d38cf069"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "\tdef __init__(self, vocab_size, max_length, num_classes):\n",
    "\t\tsuper(BiRNN, self).__init__()\n",
    "\n",
    "\t\tself.embedding = nn.Embedding(vocab_size, 128)\n",
    "\t\tself.gru = nn.GRU(128, 128, bidirectional=True, batch_first=True)\n",
    "\t\tself.fc1 = nn.Linear(256, 128)# 2 for bidirectional \n",
    "\t\tself.dropout1 = nn.Dropout(0.5)\n",
    "\t\tself.fc2 = nn.Linear(128, 64)\n",
    "\t\tself.dropout2 = nn.Dropout(0.5)\n",
    "\t\tself.batchNorm = nn.BatchNorm1d(64)\n",
    "\t\tself.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tx, _ = self.gru(x)\n",
    "\t\tx = x[:, -1, :]\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.dropout1(x)\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tx = self.dropout2(x)\n",
    "\t\tx = self.batchNorm(x)\n",
    "\t\tx = self.fc3(x)\n",
    "\t\tx = F.softmax(x, dim=1)\n",
    "\t\treturn x\n",
    "\n",
    "# Instantiate the model\n",
    "vocab_size = 10000\n",
    "max_length = 100\n",
    "num_classes = 10\n",
    "\n",
    "model = BiRNN(vocab_size, max_length, num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T07:05:27.022916400Z",
     "start_time": "2023-08-06T07:05:27.000544400Z"
    }
   },
   "id": "9149f80c6422efd7"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "# Preprocess the text: tokenize and encode\n",
    "def tokenize_text(text):\n",
    "\treturn word_tokenize(text)\n",
    "\n",
    "tokens = []\n",
    "for _, row in df_train.iterrows():\n",
    "\ttokens.extend(tokenize_text(row['texts']))\n",
    "\n",
    "counter = Counter(tokens)\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(counter.most_common(vocab_size-2))} # -2 for <unk> and <pad>\n",
    "vocab['<pad>'] = 0\n",
    "vocab['<unk>'] = 1\n",
    "\n",
    "def encode_text(text):\n",
    "\treturn [vocab.get(token, vocab['<unk>']) for token in tokenize_text(text)] # vocab['<unk>'] is for unknown tokens\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df_train['label'] = le.fit_transform(df_train['label'])\n",
    "df_valid['label'] = le.transform(df_valid['label'])\n",
    "df_test['label'] = le.transform(df_test['label'])\n",
    "\n",
    "# Create custom dataset\n",
    "class TextDataset(Dataset):\n",
    "\tdef __init__(self, df):\n",
    "\t\tself.df = df\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.df)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\ttext = self.df.iloc[idx]['texts']\n",
    "\t\tlabel = self.df.iloc[idx]['label']\n",
    "\t\treturn torch.tensor(encode_text(text)), torch.tensor(label)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_data = TextDataset(df_train)\n",
    "valid_data = TextDataset(df_valid)\n",
    "test_data = TextDataset(df_test)\n",
    "\n",
    "def collate_batch(batch):\n",
    "\tlabel_list, text_list, lengths = [], [], []\n",
    "\tfor (_text, _label) in batch:\n",
    "\t\tlabel_list.append(_label)\n",
    "\t\ttext_list.append(_text)\n",
    "\t\tlengths.append(len(_text))\n",
    "\tlabel_list = torch.tensor(label_list)\n",
    "\ttext_list = pad_sequence(text_list, padding_value=vocab['<pad>'])\n",
    "\tlengths = torch.tensor(lengths)\n",
    "\treturn text_list.transpose(0, 1), label_list, lengths\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# Training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "\tepoch_loss = 0\n",
    "\tepoch_acc = 0\n",
    "\tmodel.train()\n",
    "\tfor texts, labels, _ in iterator:  # ignore lengths\n",
    "\t\ttexts, labels = texts.to(device), labels.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = model(texts)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\t_, preds = torch.max(outputs, 1)\n",
    "\t\tcorrect = (preds == labels).float()\n",
    "\t\tacc = correct.sum() / len(correct)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tepoch_loss += loss.item()\n",
    "\t\tepoch_acc += acc.item()\n",
    "\treturn epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "\ttrain_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "\tprint(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T07:28:02.932139700Z",
     "start_time": "2023-08-06T07:28:00.001311200Z"
    }
   },
   "id": "edbdfc1dd87ad472"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# check if i have cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T07:27:28.064261300Z",
     "start_time": "2023-08-06T07:27:28.040747700Z"
    }
   },
   "id": "ada84fb6c63602bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\tepoch_loss = 0\n",
    "\tepoch_acc = 0\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor texts, labels, _ in iterator:  # ignore lengths\n",
    "\t\t\ttexts, labels = texts.to(device), labels.to(device)\n",
    "\t\t\toutputs = model(texts)\n",
    "\t\t\tloss = criterion(outputs, labels)\n",
    "\t\t\t_, preds = torch.max(outputs, 1)\n",
    "\t\t\tcorrect = (preds == labels).float()\n",
    "\t\t\tacc = correct.sum() / len(correct)\n",
    "\t\t\tepoch_loss += loss.item()\n",
    "\t\t\tepoch_acc += acc.item()\n",
    "\treturn epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
    "print(f'Valid Loss: {valid_loss:.3f}, Valid Acc: {valid_acc*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31eaf4f201ab65ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GZIP Implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f92c56aae286b4de"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   texts  label\n0      ไปจองมาแล้วนาจา Mitsubishi Attrage ได้หลังสงกร...      1\n1      เปิดศักราชใหม่! นายกฯ แถลงข่าวก่อนการแข่งขันศึ...      1\n2                               บัตรสมาชิกลดได้อีกไหมคับ      1\n3                                    สนใจ new mazda2ครับ      1\n4                                                     😍😍      0\n...                                                  ...    ...\n21623                                ไม่ค่อยอยากกินเล๊ย💘      0\n21624                               คิดถึงแม่รุ้งอีกแล้ว      0\n21625  วันนี้อะไปลองมาละบลัช 4u2 สีที่จะเอาหมดอีก โอย...      2\n21626                     ตัวอยู่พฤกษาใจไปแสนสิริ5555555      1\n21627  อย่าช้า มาทดลองขับ Mu-x เอกสิทธิ์พิเศษ รับซัมเ...      1\n\n[21605 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>texts</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ไปจองมาแล้วนาจา Mitsubishi Attrage ได้หลังสงกร...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>เปิดศักราชใหม่! นายกฯ แถลงข่าวก่อนการแข่งขันศึ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>บัตรสมาชิกลดได้อีกไหมคับ</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>สนใจ new mazda2ครับ</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>😍😍</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21623</th>\n      <td>ไม่ค่อยอยากกินเล๊ย💘</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21624</th>\n      <td>คิดถึงแม่รุ้งอีกแล้ว</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21625</th>\n      <td>วันนี้อะไปลองมาละบลัช 4u2 สีที่จะเอาหมดอีก โอย...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>21626</th>\n      <td>ตัวอยู่พฤกษาใจไปแสนสิริ5555555</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21627</th>\n      <td>อย่าช้า มาทดลองขับ Mu-x เอกสิทธิ์พิเศษ รับซัมเ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>21605 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove short word from texts\n",
    "df_train['texts'] = df_train['texts'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# drop empty texts\n",
    "df_train = df_train[df_train['texts'].map(len) > 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T07:09:04.516930300Z",
     "start_time": "2023-08-06T07:09:04.494931200Z"
    }
   },
   "id": "f72df1b6d36328a0"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2669 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "21623",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31m_RemoteTraceback\u001B[0m                          Traceback (most recent call last)",
      "\u001B[1;31m_RemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\rthaw\\anaconda3\\envs\\KDAI\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\rthaw\\anaconda3\\envs\\KDAI\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\rthaw\\anaconda3\\envs\\KDAI\\lib\\site-packages\\joblib\\parallel.py\", line 588, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\rthaw\\anaconda3\\envs\\KDAI\\lib\\site-packages\\joblib\\parallel.py\", line 588, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\rthaw\\AppData\\Local\\Temp\\ipykernel_22684\\2927837138.py\", line 6, in process_dataset_subset\nKeyError: 21623\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[120], line 53\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# parallelize iteration over training set into num_processes chunks\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mnum_processes, backend\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloky\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m parallel:\n\u001B[1;32m---> 53\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_dataset_subset\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_train\u001B[49m\u001B[43m[\u001B[49m\u001B[43mrange_start\u001B[49m\u001B[43m:\u001B[49m\u001B[43mrange_end\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc_test_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrange_start\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrange_end\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mranges\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m results:\n\u001B[0;32m     58\u001B[0m         all_train_distances_to_test\u001B[38;5;241m.\u001B[39mextend(p)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KDAI\\lib\\site-packages\\joblib\\parallel.py:1944\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1938\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   1939\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   1940\u001B[0m \u001B[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001B[39;00m\n\u001B[0;32m   1941\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   1942\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1944\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KDAI\\lib\\site-packages\\joblib\\parallel.py:1587\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1584\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1586\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1587\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1589\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1590\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1591\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1592\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1593\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KDAI\\lib\\site-packages\\joblib\\parallel.py:1691\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1684\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_retrieval():\n\u001B[0;32m   1685\u001B[0m \n\u001B[0;32m   1686\u001B[0m     \u001B[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001B[39;00m\n\u001B[0;32m   1687\u001B[0m     \u001B[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001B[39;00m\n\u001B[0;32m   1688\u001B[0m     \u001B[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001B[39;00m\n\u001B[0;32m   1689\u001B[0m     \u001B[38;5;66;03m# worker traceback.\u001B[39;00m\n\u001B[0;32m   1690\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_aborting:\n\u001B[1;32m-> 1691\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_error_fast\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1692\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1694\u001B[0m     \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1695\u001B[0m     \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KDAI\\lib\\site-packages\\joblib\\parallel.py:1726\u001B[0m, in \u001B[0;36mParallel._raise_error_fast\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1722\u001B[0m \u001B[38;5;66;03m# If this error job exists, immediatly raise the error by\u001B[39;00m\n\u001B[0;32m   1723\u001B[0m \u001B[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001B[39;00m\n\u001B[0;32m   1724\u001B[0m \u001B[38;5;66;03m# called directly or if the generator is gc'ed.\u001B[39;00m\n\u001B[0;32m   1725\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error_job \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1726\u001B[0m     \u001B[43merror_job\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KDAI\\lib\\site-packages\\joblib\\parallel.py:735\u001B[0m, in \u001B[0;36mBatchCompletionCallBack.get_result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    729\u001B[0m backend \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparallel\u001B[38;5;241m.\u001B[39m_backend\n\u001B[0;32m    731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m backend\u001B[38;5;241m.\u001B[39msupports_retrieve_callback:\n\u001B[0;32m    732\u001B[0m     \u001B[38;5;66;03m# We assume that the result has already been retrieved by the\u001B[39;00m\n\u001B[0;32m    733\u001B[0m     \u001B[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001B[39;00m\n\u001B[0;32m    734\u001B[0m     \u001B[38;5;66;03m# be returned.\u001B[39;00m\n\u001B[1;32m--> 735\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_return_or_raise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    737\u001B[0m \u001B[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001B[39;00m\n\u001B[0;32m    738\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KDAI\\lib\\site-packages\\joblib\\parallel.py:753\u001B[0m, in \u001B[0;36mBatchCompletionCallBack._return_or_raise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    751\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    752\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m==\u001B[39m TASK_ERROR:\n\u001B[1;32m--> 753\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n\u001B[0;32m    754\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[1;31mKeyError\u001B[0m: 21623"
     ]
    }
   ],
   "source": [
    "def process_dataset_subset(df_train_subset, test_text, c_test_text, d):\n",
    "\tdistances_to_test = []\n",
    "\tfor row_train in df_train_subset.iterrows():\n",
    "\t\tindex = row_train[0]\n",
    "\t\ttrain_text = row_train[1][\"texts\"]\n",
    "\t\tc_train_text = d[index]\n",
    "\n",
    "\t\ttrain_plus_test = \" \".join([test_text, train_text])\n",
    "\t\tc_train_plus_test = len(gzip.compress(train_plus_test.encode()))\n",
    "\n",
    "\t\tncd = ( (c_train_plus_test - min(c_train_text, c_test_text))\n",
    "\t\t        / max(c_test_text, c_train_text) )\n",
    "\n",
    "\t\tdistances_to_test.append(ncd)\n",
    "\n",
    "\treturn distances_to_test\n",
    "\n",
    "\n",
    "def divide_range_into_chunks(start, end, num_chunks):\n",
    "\tchunk_size = (end - start) // num_chunks\n",
    "\tranges = [(i, i + chunk_size) for i in range(start, end, chunk_size)]\n",
    "\tranges[-1] = (ranges[-1][0], end)  # Ensure the last chunk includes the end\n",
    "\treturn ranges\n",
    "\n",
    "num_processes = mp.cpu_count()\n",
    "k = 2\n",
    "predicted_classes = []\n",
    "\n",
    "start = 0\n",
    "end = df_train.shape[0]\n",
    "ranges = divide_range_into_chunks(start, end, num_chunks=num_processes)\n",
    "\n",
    "\n",
    "# caching compressed training examples\n",
    "d = {}\n",
    "for i, row_train in enumerate(df_train.iterrows()):\n",
    "    train_text = row_train[1][\"texts\"]\n",
    "    train_label = row_train[1][\"label\"]\n",
    "    c_train_text = len(gzip.compress(train_text.encode()))\n",
    "    d[i] = c_train_text\n",
    "\n",
    "# main loop\n",
    "for row_test in tqdm(df_test.iterrows(), total=df_test.shape[0]):\n",
    "\n",
    "    test_text = row_test[1][\"texts\"]\n",
    "    test_label = row_test[1][\"label\"]\n",
    "    c_test_text = len(gzip.compress(test_text.encode()))\n",
    "    all_train_distances_to_test = []\n",
    "\n",
    "    # parallelize iteration over training set into num_processes chunks\n",
    "    with Parallel(n_jobs=num_processes, backend=\"loky\") as parallel:\n",
    "\n",
    "        results = parallel(\n",
    "            delayed(process_dataset_subset)(df_train[range_start:range_end], test_text, c_test_text, d)\n",
    "            for range_start, range_end in ranges\n",
    "        )\n",
    "        for p in results:\n",
    "            all_train_distances_to_test.extend(p)\n",
    "\n",
    "    sorted_idx = np.argsort(np.array(all_train_distances_to_test.extend))\n",
    "    top_k_class = np.array(df_train[\"label\"])[sorted_idx[:k]]\n",
    "    predicted_class = Counter(top_k_class).most_common()[0][0]\n",
    "    predicted_classes.append(predicted_class)\n",
    "\n",
    "print(\"Accuracy:\", np.mean(np.array(predicted_classes) == df_test[\"label\"].values))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T07:09:16.655806Z",
     "start_time": "2023-08-06T07:09:13.034425500Z"
    }
   },
   "id": "a99ec000c17fbee7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
